{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11420152,"sourceType":"datasetVersion","datasetId":7152221}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Imports\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras.applications import VGG16, ResNet50, DenseNet121, EfficientNetB3\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Input, BatchNormalization\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.model_selection import KFold\nimport pickle\nimport time\nimport collections\nfrom sklearn.metrics import (\n    accuracy_score, roc_auc_score, precision_score, recall_score,\n    f1_score, confusion_matrix, roc_curve\n)\nfrom skopt import gp_minimize\nfrom skopt.space import Real, Integer, Categorical\nfrom skopt.utils import use_named_args","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-26T09:57:53.169958Z","iopub.execute_input":"2025-04-26T09:57:53.170930Z","iopub.status.idle":"2025-04-26T09:58:06.931142Z","shell.execute_reply.started":"2025-04-26T09:57:53.170903Z","shell.execute_reply":"2025-04-26T09:58:06.930358Z"}},"outputs":[{"name":"stderr","text":"2025-04-26 09:57:54.763986: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745661474.957287      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745661475.012661      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Use Mixed Precision (save VRAM)\nfrom tensorflow.keras import mixed_precision\nmixed_precision.set_global_policy(\"mixed_float16\")\nprint(\"mixed precision enabled.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T09:58:06.932384Z","iopub.execute_input":"2025-04-26T09:58:06.932853Z","iopub.status.idle":"2025-04-26T09:58:06.937269Z","shell.execute_reply.started":"2025-04-26T09:58:06.932832Z","shell.execute_reply":"2025-04-26T09:58:06.936402Z"}},"outputs":[{"name":"stdout","text":"mixed precision enabled.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Load Preprocessed Data --- balanced checked\nDATA_PATH = \"/kaggle/input/preprocessed-mammo-splits\"  \ntrain = np.load(os.path.join(DATA_PATH, \"train_data.npz\"))\nval = np.load(os.path.join(DATA_PATH, \"val_data.npz\"))\ntest = np.load(os.path.join(DATA_PATH, \"test_data.npz\"))\n\nX_train, y_train = train[\"X\"], train[\"y\"]\nX_val, y_val = val[\"X\"], val[\"y\"]\nX_test, y_test = test[\"X\"], test[\"y\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T09:58:06.938191Z","iopub.execute_input":"2025-04-26T09:58:06.938453Z","iopub.status.idle":"2025-04-26T09:58:38.959098Z","shell.execute_reply.started":"2025-04-26T09:58:06.938430Z","shell.execute_reply":"2025-04-26T09:58:38.958504Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Compute Class Weights\nclass_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\nclass_weight_dict = dict(zip(np.unique(y_train), class_weights))\nprint(\"Class Weights:\", class_weight_dict)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T09:58:38.960407Z","iopub.execute_input":"2025-04-26T09:58:38.960677Z","iopub.status.idle":"2025-04-26T09:58:38.971859Z","shell.execute_reply.started":"2025-04-26T09:58:38.960658Z","shell.execute_reply":"2025-04-26T09:58:38.971201Z"}},"outputs":[{"name":"stdout","text":"Class Weights: {0: 1.1308917197452228, 1: 0.8962645128722867}\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Expand dims because TF expects (H, W, 1) from (H, W)\nX_train = X_train[..., np.newaxis].astype(\"float32\")\nX_val = X_val[..., np.newaxis].astype(\"float32\")\nX_test = X_test[..., np.newaxis].astype(\"float32\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T09:58:38.972492Z","iopub.execute_input":"2025-04-26T09:58:38.972714Z","iopub.status.idle":"2025-04-26T09:58:40.899749Z","shell.execute_reply.started":"2025-04-26T09:58:38.972699Z","shell.execute_reply":"2025-04-26T09:58:40.899188Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Enhanced data augmentation\ndef convert_to_rgb(image, label):\n    image_rgb = tf.image.grayscale_to_rgb(image)  \n    image_rgb = tf.squeeze(image_rgb) \n    return image_rgb, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T09:58:40.900430Z","iopub.execute_input":"2025-04-26T09:58:40.900647Z","iopub.status.idle":"2025-04-26T09:58:40.904326Z","shell.execute_reply.started":"2025-04-26T09:58:40.900631Z","shell.execute_reply":"2025-04-26T09:58:40.903595Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def augment(image, label):\n    # Random rotation (0-15 degrees)\n    angle = tf.random.uniform([], -0.26, 0.26)  # ~15 degrees in radians\n    image = tf.image.rot90(image, k=tf.cast(angle * 2 / 3.14159, tf.int32))\n    \n    # Random flips\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_flip_up_down(image)\n    \n    # Random brightness/contrast adjustments\n    image = tf.image.random_brightness(image, 0.2)\n    image = tf.image.random_contrast(image, 0.8, 1.2)\n    \n    # Random zoom (crop and resize)\n    zoom_factor = tf.random.uniform([], 0.8, 1.0, dtype=tf.float32)\n    h, w = tf.shape(image)[0], tf.shape(image)[1]\n    crop_size_h = tf.cast(tf.cast(h, tf.float32) * zoom_factor, tf.int32)\n    crop_size_w = tf.cast(tf.cast(w, tf.float32) * zoom_factor, tf.int32)\n    \n    # Ensure crop dimensions don't exceed image dimensions\n    crop_size_h = tf.minimum(crop_size_h, h)\n    crop_size_w = tf.minimum(crop_size_w, w)\n    \n    image = tf.image.random_crop(image, size=[crop_size_h, crop_size_w, 3])\n    image = tf.image.resize(image, [224, 224])\n    \n    return image, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T09:58:40.905124Z","iopub.execute_input":"2025-04-26T09:58:40.905373Z","iopub.status.idle":"2025-04-26T09:58:40.932223Z","shell.execute_reply.started":"2025-04-26T09:58:40.905352Z","shell.execute_reply":"2025-04-26T09:58:40.931565Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"BATCH_SIZE = 32\nAUTOTUNE = tf.data.AUTOTUNE\n\n# Create datasets\ntrain_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\nval_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val))\ntest_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n\n# Apply preprocessing and augmentation\ntrain_ds = (\n    train_ds.shuffle(1024)\n    .map(convert_to_rgb, num_parallel_calls=AUTOTUNE)\n    .map(augment, num_parallel_calls=AUTOTUNE)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTOTUNE)\n)\n\nval_ds = (\n    val_ds.map(convert_to_rgb, num_parallel_calls=AUTOTUNE)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTOTUNE)\n)\n\ntest_ds = (\n    test_ds.map(convert_to_rgb, num_parallel_calls=AUTOTUNE)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTOTUNE)\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T09:58:40.932958Z","iopub.execute_input":"2025-04-26T09:58:40.933163Z","iopub.status.idle":"2025-04-26T09:58:54.136757Z","shell.execute_reply.started":"2025-04-26T09:58:40.933141Z","shell.execute_reply":"2025-04-26T09:58:54.135988Z"}},"outputs":[{"name":"stderr","text":"I0000 00:00:1745661523.106580      31 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Bayesian Optimization Space\nbayes_space = [\n    Real(1e-5, 1e-3, \"log-uniform\", name=\"learning_rate\"),\n    Integer(128, 1024, name=\"dense_units_1\"),\n    Integer(64, 512, name=\"dense_units_2\"),\n    Real(0.2, 0.7, name=\"dropout_rate_1\"),\n    Real(0.1, 0.5, name=\"dropout_rate_2\"),\n    Integer(10, 50, name=\"unfreeze_layers\")\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T09:58:54.137592Z","iopub.execute_input":"2025-04-26T09:58:54.138038Z","iopub.status.idle":"2025-04-26T09:58:54.145273Z","shell.execute_reply.started":"2025-04-26T09:58:54.138017Z","shell.execute_reply":"2025-04-26T09:58:54.144514Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def build_tuned_model(base_model_fn, hyperparams, name=\"model\"):\n    base_model = base_model_fn(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n    base_model.trainable = False\n    \n    inputs = Input(shape=(224, 224, 3))\n    x = base_model(inputs, training=False)\n    x = GlobalAveragePooling2D()(x)\n    \n    x = Dense(hyperparams[\"dense_units_1\"], activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(hyperparams[\"dropout_rate_1\"])(x)\n    \n    x = Dense(hyperparams[\"dense_units_2\"], activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(hyperparams[\"dropout_rate_2\"])(x)\n    \n    outputs = Dense(1, activation='sigmoid', dtype='float32')(x)\n    \n    model = Model(inputs, outputs, name=name)\n\n    model.compile(\n        optimizer=Adam(learning_rate=hyperparams[\"learning_rate\"]),\n        loss='binary_crossentropy',\n        metrics=[\n            'accuracy', \n            tf.keras.metrics.AUC(name='auc'),\n            tf.keras.metrics.Precision(name='precision'),\n            tf.keras.metrics.Recall(name='recall')\n        ]\n    )\n    \n    return model, base_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T09:58:54.147709Z","iopub.execute_input":"2025-04-26T09:58:54.147914Z","iopub.status.idle":"2025-04-26T09:58:54.164895Z","shell.execute_reply.started":"2025-04-26T09:58:54.147899Z","shell.execute_reply":"2025-04-26T09:58:54.164250Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def unfreeze_tuned_model(model, base_model, hyperparams):\n    base_model.trainable = True\n    \n    for layer in base_model.layers[:-hyperparams[\"unfreeze_layers\"]]:\n        layer.trainable = False\n    \n    model.compile(\n        optimizer=Adam(learning_rate=hyperparams[\"learning_rate\"]/10),\n        loss='binary_crossentropy',\n        metrics=[\n            'accuracy', \n            tf.keras.metrics.AUC(name='auc',dtype=tf.float32),\n            tf.keras.metrics.Precision(name='precision'),\n            tf.keras.metrics.Recall(name='recall')\n        ]\n    )\n    \n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T09:58:54.165430Z","iopub.execute_input":"2025-04-26T09:58:54.165635Z","iopub.status.idle":"2025-04-26T09:58:54.179792Z","shell.execute_reply.started":"2025-04-26T09:58:54.165621Z","shell.execute_reply":"2025-04-26T09:58:54.179077Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"@use_named_args(bayes_space)\ndef objective_function(**params):\n    hyperparams = {\n        \"learning_rate\": params[\"learning_rate\"],\n        \"dense_units_1\": params[\"dense_units_1\"],\n        \"dense_units_2\": params[\"dense_units_2\"],\n        \"dropout_rate_1\": params[\"dropout_rate_1\"],\n        \"dropout_rate_2\": params[\"dropout_rate_2\"],\n        \"unfreeze_layers\": params[\"unfreeze_layers\"]\n    }\n    \n    model_fn = VGG16\n    \n    model, base_model = build_tuned_model(model_fn, hyperparams, name=\"bayes_opt_model\")\n    \n    tuning_callbacks = [\n        EarlyStopping(monitor='val_auc', patience=3, restore_best_weights=True),\n        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-6)\n    ]\n    \n    model.fit(\n        train_ds,\n        validation_data=val_ds,\n        epochs=3,\n        class_weight=class_weight_dict,\n        callbacks=tuning_callbacks,\n        verbose=0\n    )\n    \n    model = unfreeze_tuned_model(model, base_model, hyperparams)\n    model.fit(\n        train_ds,\n        validation_data=val_ds,\n        epochs=3,\n        class_weight=class_weight_dict,\n        callbacks=tuning_callbacks,\n        verbose=0\n    )\n    \n    val_results = model.evaluate(val_ds, verbose=0,return_dict=True)\n    print(\"Evaluation metrics:\", val_results.keys())\n    val_auc = val_results['auc']\n    \n    return -val_auc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T09:58:54.180716Z","iopub.execute_input":"2025-04-26T09:58:54.180970Z","iopub.status.idle":"2025-04-26T09:58:54.193274Z","shell.execute_reply.started":"2025-04-26T09:58:54.180949Z","shell.execute_reply":"2025-04-26T09:58:54.192679Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def run_bayesian_optimization(model_fn, n_calls=1):\n    print(f\"Starting Bayesian Optimization with {n_calls} iterations...\")\n    \n    result = gp_minimize(\n        objective_function,\n        bayes_space,\n        n_calls=n_calls,\n        n_initial_points=1,\n        verbose=True\n    )\n    \n    best_params = {\n        \"learning_rate\": result.x[0],\n        \"dense_units_1\": result.x[1],\n        \"dense_units_2\": result.x[2],\n        \"dropout_rate_1\": result.x[3],\n        \"dropout_rate_2\": result.x[4],\n        \"unfreeze_layers\": result.x[5]\n    }\n    \n    print(\"\\nBest parameters found:\")\n    for param, value in best_params.items():\n        print(f\"{param}: {value}\")\n    \n    print(f\"Best validation AUC: {-result.fun:.4f}\")\n    \n    return best_params","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T09:58:54.193893Z","iopub.execute_input":"2025-04-26T09:58:54.194122Z","iopub.status.idle":"2025-04-26T09:58:54.210443Z","shell.execute_reply.started":"2025-04-26T09:58:54.194102Z","shell.execute_reply":"2025-04-26T09:58:54.209875Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def run_hyperparameter_optimization(model_fn, name, n_calls=1):\n    print(f\"\\n{'='*50}\")\n    print(f\"Bayesian optimization for {name}...\")\n    print(f\"{'='*50}\")\n    \n    best_params = run_bayesian_optimization(model_fn, n_calls=n_calls)\n    model, base_model = build_tuned_model(model_fn, best_params, name=f\"{name}_bayes_opt\")\n    \n    with open(f\"{name}_best_hyperparams.pkl\", \"wb\") as f:\n        pickle.dump(best_params, f)\n    print(f\"Saved hyperparameters: {name}_best_hyperparams.pkl\")\n    \n    return model, base_model, best_params","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T09:58:54.211136Z","iopub.execute_input":"2025-04-26T09:58:54.211335Z","iopub.status.idle":"2025-04-26T09:58:54.228191Z","shell.execute_reply.started":"2025-04-26T09:58:54.211315Z","shell.execute_reply":"2025-04-26T09:58:54.227524Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def find_optimal_threshold(model, ds):\n    # Get predictions and true labels\n    pred = model.predict(ds)\n    true = np.concatenate([y for x, y in ds], axis=0)\n    \n    # Calculate ROC curve\n    fpr, tpr, thresholds = roc_curve(true, pred)\n    \n    # Find optimal threshold using Youden's J statistic\n    j_scores = tpr - fpr\n    optimal_idx = np.argmax(j_scores)\n    optimal_threshold = thresholds[optimal_idx]\n    \n    print(f\"Optimal threshold: {optimal_threshold:.4f}\")\n    return optimal_threshold","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T09:58:54.228777Z","iopub.execute_input":"2025-04-26T09:58:54.228998Z","iopub.status.idle":"2025-04-26T09:58:54.242849Z","shell.execute_reply.started":"2025-04-26T09:58:54.228979Z","shell.execute_reply":"2025-04-26T09:58:54.242176Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"def evaluate_with_threshold(model, ds, threshold=0.5):\n    # Get predictions\n    pred = model.predict(ds)\n    \n    # Get true labels\n    true = np.concatenate([y for x, y in ds], axis=0)\n    \n    # Apply threshold\n    pred_binary = (pred > threshold).astype(int)\n    \n    # Calculate metrics\n    acc = accuracy_score(true, pred_binary)\n    auc = roc_auc_score(true, pred)\n    precision = precision_score(true, pred_binary)\n    recall = recall_score(true, pred_binary)\n    f1 = f1_score(true, pred_binary)\n    cm = confusion_matrix(true, pred_binary)\n    \n    # Calculate specificity\n    tn, fp, fn, tp = cm.ravel()\n    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n    \n    print(f\"\\n{'='*30} Evaluation Results {'='*30}\")\n    print(f\"Accuracy: {acc:.4f}\")\n    print(f\"AUC: {auc:.4f}\")\n    print(f\"Precision: {precision:.4f}\")\n    print(f\"Recall (Sensitivity): {recall:.4f}\")\n    print(f\"Specificity: {specificity:.4f}\")\n    print(f\"F1 Score: {f1:.4f}\")\n    print(f\"Confusion Matrix:\\n{cm}\")\n    print(f\"{'='*78}\")\n    \n    return {\n        'accuracy': acc,\n        'auc': auc,\n        'precision': precision,\n        'recall': recall,\n        'specificity': specificity,\n        'f1': f1,\n        'confusion_matrix': cm,\n        'predictions': pred,\n        'threshold': threshold\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T09:58:54.243592Z","iopub.execute_input":"2025-04-26T09:58:54.243791Z","iopub.status.idle":"2025-04-26T09:58:54.256995Z","shell.execute_reply.started":"2025-04-26T09:58:54.243778Z","shell.execute_reply":"2025-04-26T09:58:54.256312Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# train_model_with_hyperopt function\ndef train_model_with_hyperopt(name, model_fn, n_calls=1):\n    print(f\"\\n{'='*50}\")\n    print(f\"Training {name} with Bayesian optimization...\")\n    print(f\"{'='*50}\")\n    \n    model, base_model, best_params = run_hyperparameter_optimization(\n        model_fn, \n        name,\n        n_calls=n_calls\n    )\n    \n    callbacks = [\n        EarlyStopping(patience=15, restore_best_weights=True, verbose=1),\n        ModelCheckpoint(\n            f\"/kaggle/working/models/{name}_phase1.keras\",\n            save_best_only=True,\n            monitor='val_auc',\n            mode='max',\n            verbose=1\n        ),\n        ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.2,\n            patience=5,\n            min_lr=1e-6,\n            verbose=1\n        )\n    ]\n    \n    # Phase 1: Frozen base training\n    print(\"Initial training with frozen base layers...\")\n    history1 = model.fit(\n        train_ds,\n        validation_data=val_ds,\n        epochs=3,\n        class_weight=class_weight_dict,\n        callbacks=callbacks,\n        verbose=2\n    )\n    \n    # Phase 2: Fine-tuning\n    print(\"\\nFine-tuning with unfrozen layers...\")\n    model = unfreeze_tuned_model(model, base_model, best_params)\n    callbacks[1] = ModelCheckpoint(\n        f\"/kaggle/working/models/{name}_phase2.keras\",\n        save_best_only=True,\n        monitor='val_auc',\n        mode='max',\n        verbose=1\n    )\n    \n    history2 = model.fit(\n        train_ds,\n        validation_data=val_ds,\n        epochs=3,\n        class_weight=class_weight_dict,\n        callbacks=callbacks,\n        verbose=2\n    )\n    \n    # Threshold optimization and evaluation\n    print(\"\\nOptimizing classification threshold...\")\n    optimal_threshold = find_optimal_threshold(model, val_ds)\n    \n    print(\"\\nFinal evaluation on test set:\")\n    test_results = evaluate_with_threshold(model, test_ds, threshold=optimal_threshold)\n    \n    # Save model and results\n    model.save(f\"{name}_trained_model.h5\")\n    print(f\"Saved model: {name}_trained_model.h5\")\n    \n    combined_history = {\n        'phase1': history1.history,\n        'phase2': history2.history,\n        'best_hyperparams': best_params,\n        'test_results': test_results,\n        'optimal_threshold': optimal_threshold\n    }\n    \n    with open(f\"{name}_history.pkl\", \"wb\") as f:\n        pickle.dump(combined_history, f)\n    \n    return model, test_results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T09:58:54.257720Z","iopub.execute_input":"2025-04-26T09:58:54.257904Z","iopub.status.idle":"2025-04-26T09:58:54.275394Z","shell.execute_reply.started":"2025-04-26T09:58:54.257887Z","shell.execute_reply":"2025-04-26T09:58:54.274829Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# Initialize storage\nmodel_results = {}\nhistory_dict = {}\nall_trained_models = {}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T09:58:54.276025Z","iopub.execute_input":"2025-04-26T09:58:54.276670Z","iopub.status.idle":"2025-04-26T09:58:54.293628Z","shell.execute_reply.started":"2025-04-26T09:58:54.276654Z","shell.execute_reply":"2025-04-26T09:58:54.293011Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"models_to_train = {\n    \"VGG16\": VGG16\n    # \"ResNet50\": ResNet50\n    # \"DenseNet121\": DenseNet121\n    # \"EfficientNetB3\": EfficientNetB3  \n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T09:58:54.294311Z","iopub.execute_input":"2025-04-26T09:58:54.294808Z","iopub.status.idle":"2025-04-26T09:58:54.308375Z","shell.execute_reply.started":"2025-04-26T09:58:54.294786Z","shell.execute_reply":"2025-04-26T09:58:54.307829Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"for name, model_fn in models_to_train.items():\n    print(f\"\\n{'='*50}\")\n    print(f\"Training {name}...\")\n    print(f\"{'='*50}\")\n    \n    # Hyperparameter Optimization\n    print(\"Running Bayesian optimization...\")\n    model, base_model, best_params = run_hyperparameter_optimization(model_fn, name)\n    \n    # Phase 1: Frozen Base Training\n    print(\"\\nPhase 1: Initial training with frozen base layers\")\n    phase1_callbacks = [\n        EarlyStopping(monitor='val_auc', patience=15, restore_best_weights=True),\n        ModelCheckpoint(f\"{name}_phase1.keras\", monitor='val_auc', save_best_only=True, mode='max'),\n        ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6)\n    ]\n    \n    history1 = model.fit(\n        train_ds,\n        validation_data=val_ds,\n        epochs=3,\n        class_weight=class_weight_dict,\n        callbacks=phase1_callbacks,\n        verbose=2\n    )\n    \n    # Phase 2: Fine-tuning\n    print(\"\\nPhase 2: Fine-tuning with unfrozen layers\")\n    model = unfreeze_tuned_model(model, base_model, best_params)\n    \n    phase2_callbacks = [\n        EarlyStopping(monitor='val_auc', patience=10, restore_best_weights=True),\n        ModelCheckpoint(f\"{name}_phase2.keras\", monitor='val_auc', save_best_only=True, mode='max'),\n        ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-7)\n    ]\n    \n    history2 = model.fit(\n        train_ds,\n        validation_data=val_ds,\n        epochs=3,\n        class_weight=class_weight_dict,\n        callbacks=phase2_callbacks,\n        verbose=2\n    )\n    \n    # Threshold Optimization & Evaluation\n    print(\"\\nOptimizing classification threshold...\")\n    optimal_threshold = find_optimal_threshold(model, val_ds)\n    \n    print(\"\\nFinal Evaluation:\")\n    test_results = evaluate_with_threshold(model, test_ds, threshold=optimal_threshold)\n    model_results[name] = test_results\n    \n    # Save artifacts\n    model.save(f\"{name}_trained_model.keras\")\n    history_dict[name] = {\n        'phase1': history1.history,\n        'phase2': history2.history,\n        'best_params': best_params,\n        'optimal_threshold': optimal_threshold\n    }\n    with open(f\"{name}_history.pkl\", \"wb\") as f:\n        pickle.dump(history_dict[name], f)\n\n# ============== Results Summary ==============        \nprint(\"\\n\\n=== Final Results Summary ===\")\nfor model_name, results in model_results.items():\n    print(f\"\\n{model_name}:\")\n    print(f\"AUC: {results['auc']:.4f} | Accuracy: {results['accuracy']:.4f}\")\n    print(f\"Precision: {results['precision']:.4f} | Recall: {results['recall']:.4f}\")\n    print(f\"F1: {results['f1']:.4f} | Specificity: {results['specificity']:.4f}\")\n    print(f\"Optimal Threshold: {history_dict[model_name]['optimal_threshold']:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T09:58:54.309023Z","iopub.execute_input":"2025-04-26T09:58:54.309210Z","iopub.status.idle":"2025-04-26T10:18:36.382911Z","shell.execute_reply.started":"2025-04-26T09:58:54.309197Z","shell.execute_reply":"2025-04-26T10:18:36.382163Z"}},"outputs":[{"name":"stdout","text":"\n==================================================\nTraining VGG16...\n==================================================\nRunning Bayesian optimization...\n\n==================================================\nBayesian optimization for VGG16...\n==================================================\nStarting Bayesian Optimization with 1 iterations...\nIteration No: 1 started. Evaluating function at random point.\nDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n\u001b[1m58889256/58889256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1745661543.827275      90 service.cc:148] XLA service 0x784fa4002320 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1745661543.828097      90 service.cc:156]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\nI0000 00:00:1745661544.558272      90 cuda_dnn.cc:529] Loaded cuDNN version 90300\nI0000 00:00:1745661555.580393      90 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"Evaluation metrics: dict_keys(['accuracy', 'auc', 'loss', 'precision', 'recall'])\nIteration No: 1 ended. Evaluation done at random point.\nTime taken: 615.4749\nFunction value obtained: -0.9245\nCurrent minimum: -0.9245\n\nBest parameters found:\nlearning_rate: 1.10659481809529e-05\ndense_units_1: 277\ndense_units_2: 123\ndropout_rate_1: 0.5701496095020742\ndropout_rate_2: 0.18079769551281485\nunfreeze_layers: 49\nBest validation AUC: 0.9245\nSaved hyperparameters: VGG16_best_hyperparams.pkl\n\nPhase 1: Initial training with frozen base layers\nEpoch 1/3\n555/555 - 62s - 112ms/step - accuracy: 0.5316 - auc: 0.5437 - loss: 0.8479 - precision: 0.5867 - recall: 0.5426 - val_accuracy: 0.6289 - val_auc: 0.6796 - val_loss: 0.6380 - val_precision: 0.6742 - val_recall: 0.6476 - learning_rate: 1.1066e-05\nEpoch 2/3\n555/555 - 47s - 85ms/step - accuracy: 0.5720 - auc: 0.6002 - loss: 0.7700 - precision: 0.6260 - recall: 0.5781 - val_accuracy: 0.6758 - val_auc: 0.7509 - val_loss: 0.5938 - val_precision: 0.7325 - val_recall: 0.6596 - learning_rate: 1.1066e-05\nEpoch 3/3\n555/555 - 47s - 85ms/step - accuracy: 0.5999 - auc: 0.6442 - loss: 0.7187 - precision: 0.6538 - recall: 0.6012 - val_accuracy: 0.7116 - val_auc: 0.7893 - val_loss: 0.5593 - val_precision: 0.7518 - val_recall: 0.7208 - learning_rate: 1.1066e-05\n\nPhase 2: Fine-tuning with unfrozen layers\nEpoch 1/3\n555/555 - 145s - 262ms/step - accuracy: 0.6782 - auc: 0.7502 - loss: 0.6015 - precision: 0.7257 - recall: 0.6805 - val_accuracy: 0.7881 - val_auc: 0.8982 - val_loss: 0.4249 - val_precision: 0.8633 - val_recall: 0.7368 - learning_rate: 1.1066e-06\nEpoch 2/3\n555/555 - 121s - 218ms/step - accuracy: 0.7484 - auc: 0.8337 - loss: 0.4996 - precision: 0.7858 - recall: 0.7548 - val_accuracy: 0.8204 - val_auc: 0.9161 - val_loss: 0.3674 - val_precision: 0.8395 - val_recall: 0.8381 - learning_rate: 1.1066e-06\nEpoch 3/3\n555/555 - 121s - 218ms/step - accuracy: 0.7774 - auc: 0.8682 - loss: 0.4477 - precision: 0.8127 - recall: 0.7809 - val_accuracy: 0.8255 - val_auc: 0.9209 - val_loss: 0.3443 - val_precision: 0.8230 - val_recall: 0.8753 - learning_rate: 1.1066e-06\n\nOptimizing classification threshold...\n\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 75ms/step\nOptimal threshold: 0.5471\n\nFinal Evaluation:\n\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 89ms/step\n\n============================== Evaluation Results ==============================\nAccuracy: 0.8340\nAUC: 0.9249\nPrecision: 0.8564\nRecall (Sensitivity): 0.8439\nSpecificity: 0.8215\nF1 Score: 0.8501\nConfusion Matrix:\n[[1339  291]\n [ 321 1736]]\n==============================================================================\n\n\n=== Final Results Summary ===\n\nVGG16:\nAUC: 0.9249 | Accuracy: 0.8340\nPrecision: 0.8564 | Recall: 0.8439\nF1: 0.8501 | Specificity: 0.8215\nOptimal Threshold: 0.5471\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}