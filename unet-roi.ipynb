{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11420152,"sourceType":"datasetVersion","datasetId":7152221}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dropout, Conv2DTranspose, concatenate\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.optimizers import Adam\nfrom skimage import filters, morphology, measure\nfrom scipy import ndimage\nfrom tqdm import tqdm\nimport cv2","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-02T12:56:29.555008Z","iopub.execute_input":"2025-05-02T12:56:29.555903Z","iopub.status.idle":"2025-05-02T12:56:39.372498Z","shell.execute_reply.started":"2025-05-02T12:56:29.555865Z","shell.execute_reply":"2025-05-02T12:56:39.371816Z"}},"outputs":[{"name":"stderr","text":"2025-05-02 12:56:31.179095: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746190591.401151     936 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746190591.463849     936 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Use Mixed Precision (save VRAM)\nfrom tensorflow.keras import mixed_precision\nmixed_precision.set_global_policy(\"mixed_float16\")\nprint(\"mixed precision enabled.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T12:56:39.373949Z","iopub.execute_input":"2025-05-02T12:56:39.374667Z","iopub.status.idle":"2025-05-02T12:56:39.379689Z","shell.execute_reply.started":"2025-05-02T12:56:39.374642Z","shell.execute_reply":"2025-05-02T12:56:39.378972Z"}},"outputs":[{"name":"stdout","text":"mixed precision enabled.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Set random seeds for reproducibility\nSEED = 42\nnp.random.seed(SEED)\ntf.random.set_seed(SEED)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T12:56:39.380810Z","iopub.execute_input":"2025-05-02T12:56:39.381143Z","iopub.status.idle":"2025-05-02T12:56:39.401169Z","shell.execute_reply.started":"2025-05-02T12:56:39.381108Z","shell.execute_reply":"2025-05-02T12:56:39.400277Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# ----- MEMORY OPTIMIZATIONS -----\n# Enable memory growth to avoid allocating all GPU memory at once\nphysical_devices = tf.config.list_physical_devices('GPU')\nif physical_devices:\n    for device in physical_devices:\n        tf.config.experimental.set_memory_growth(device, True)\n    print(\"Memory growth enabled for GPUs\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T12:56:39.402929Z","iopub.execute_input":"2025-05-02T12:56:39.403239Z","iopub.status.idle":"2025-05-02T12:56:40.638680Z","shell.execute_reply.started":"2025-05-02T12:56:39.403217Z","shell.execute_reply":"2025-05-02T12:56:40.637815Z"}},"outputs":[{"name":"stdout","text":"Memory growth enabled for GPUs\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Set TensorFlow to only allocate necessary memory\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        # Memory growth needs to be set before GPUs have been initialized\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n        print(f\"{len(gpus)} Physical GPUs, {len(logical_gpus)} Logical GPUs\")\n    except RuntimeError as e:\n        # Memory growth must be set before GPUs have been initialized\n        print(e)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T12:56:40.639813Z","iopub.execute_input":"2025-05-02T12:56:40.640153Z","iopub.status.idle":"2025-05-02T12:56:40.787806Z","shell.execute_reply.started":"2025-05-02T12:56:40.640124Z","shell.execute_reply":"2025-05-02T12:56:40.786728Z"}},"outputs":[{"name":"stdout","text":"1 Physical GPUs, 1 Logical GPUs\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1746190600.782532     936 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Parameters\nIMG_SIZE = 224  \nBATCH_SIZE = 4\nEPOCHS = 2\nDATASET_ROOT = '/kaggle/input/preprocessed-mammo-splits'\nOUTPUT_DIR = './cell_roi_extracted'\nSEGMENTATION_MODEL_PATH = './cell_unet_model.keras'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T12:56:40.788711Z","iopub.execute_input":"2025-05-02T12:56:40.789057Z","iopub.status.idle":"2025-05-02T12:56:40.805037Z","shell.execute_reply.started":"2025-05-02T12:56:40.789034Z","shell.execute_reply":"2025-05-02T12:56:40.804013Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Create output directory if it doesn't exist\nos.makedirs(OUTPUT_DIR, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T12:56:40.805863Z","iopub.execute_input":"2025-05-02T12:56:40.806128Z","iopub.status.idle":"2025-05-02T12:56:40.823204Z","shell.execute_reply.started":"2025-05-02T12:56:40.806108Z","shell.execute_reply":"2025-05-02T12:56:40.822363Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def iou_metric(y_true, y_pred):\n    # Ensure consistent data types by casting both to float32\n    y_true = tf.cast(y_true, tf.float32)\n    y_pred = tf.cast(y_pred > 0.5, tf.float32)\n    \n    intersection = tf.reduce_sum(y_true * y_pred)\n    union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection\n    return intersection / (union + tf.keras.backend.epsilon())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T12:56:40.824149Z","iopub.execute_input":"2025-05-02T12:56:40.824538Z","iopub.status.idle":"2025-05-02T12:56:40.841898Z","shell.execute_reply.started":"2025-05-02T12:56:40.824470Z","shell.execute_reply":"2025-05-02T12:56:40.840924Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Build U-Net model for cell segmentation\ndef build_unet_model(input_size=(IMG_SIZE, IMG_SIZE, 1)):\n    inputs = Input(input_size)\n    \n    # Encoder (Downsampling)\n    conv1 = Conv2D(32, 3, activation='relu', padding='same')(inputs)\n    conv1 = Conv2D(32, 3, activation='relu', padding='same')(conv1)\n    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n    \n    conv2 = Conv2D(64, 3, activation='relu', padding='same')(pool1)\n    conv2 = Conv2D(64, 3, activation='relu', padding='same')(conv2)\n    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n    \n    conv3 = Conv2D(128, 3, activation='relu', padding='same')(pool2)\n    conv3 = Conv2D(128, 3, activation='relu', padding='same')(conv3)\n    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n    \n    conv4 = Conv2D(256, 3, activation='relu', padding='same')(pool3)\n    conv4 = Conv2D(256, 3, activation='relu', padding='same')(conv4)\n    drop4 = Dropout(0.5)(conv4)\n    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n    \n    # Bridge\n    conv5 = Conv2D(512, 3, activation='relu', padding='same')(pool4)\n    conv5 = Conv2D(512, 3, activation='relu', padding='same')(conv5)\n    drop5 = Dropout(0.5)(conv5)\n    \n    # Decoder (Upsampling)\n    up6 = Conv2DTranspose(512, 2, strides=(2, 2), padding='same')(drop5)\n    merge6 = concatenate([drop4, up6], axis=3)\n    conv6 = Conv2D(256, 3, activation='relu', padding='same')(merge6)\n    conv6 = Conv2D(256, 3, activation='relu', padding='same')(conv6)\n    \n    up7 = Conv2DTranspose(256, 2, strides=(2, 2), padding='same')(conv6)\n    merge7 = concatenate([conv3, up7], axis=3)\n    conv7 = Conv2D(128, 3, activation='relu', padding='same')(merge7)\n    conv7 = Conv2D(128, 3, activation='relu', padding='same')(conv7)\n    \n    up8 = Conv2DTranspose(128, 2, strides=(2, 2), padding='same')(conv7)\n    merge8 = concatenate([conv2, up8], axis=3)\n    conv8 = Conv2D(64, 3, activation='relu', padding='same')(merge8)\n    conv8 = Conv2D(64, 3, activation='relu', padding='same')(conv8)\n    \n    up9 = Conv2DTranspose(64, 2, strides=(2, 2), padding='same')(conv8)\n    merge9 = concatenate([conv1, up9], axis=3)\n    conv9 = Conv2D(32, 3, activation='relu', padding='same')(merge9)\n    conv9 = Conv2D(32, 3, activation='relu', padding='same')(conv9)\n    \n    # Output layer - sigmoid for binary segmentation\n    outputs = Conv2D(1, 1, activation='sigmoid')(conv9)\n    \n    model = Model(inputs, outputs)\n    model.compile(optimizer=Adam(learning_rate=1e-4), \n                  loss='binary_crossentropy', \n                  metrics=['accuracy', iou_metric])\n    \n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T12:56:40.843092Z","iopub.execute_input":"2025-05-02T12:56:40.843372Z","iopub.status.idle":"2025-05-02T12:56:40.867028Z","shell.execute_reply.started":"2025-05-02T12:56:40.843351Z","shell.execute_reply":"2025-05-02T12:56:40.866094Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def load_preprocessed_data():\n    train_data = np.load(os.path.join(DATASET_ROOT, 'train_data.npz'))\n    val_data = np.load(os.path.join(DATASET_ROOT, 'val_data.npz'))\n    test_data = np.load(os.path.join(DATASET_ROOT, 'test_data.npz'))\n    \n    X_train, y_train = train_data['X'], train_data['y']\n    X_val, y_val = val_data['X'], val_data['y']\n    X_test, y_test = test_data['X'], test_data['y']\n    \n    # Ensure images are in grayscale for segmentation\n    if X_train.shape[-1] == 3:\n        X_train = np.mean(X_train, axis=-1, keepdims=True)\n        X_val = np.mean(X_val, axis=-1, keepdims=True)\n        X_test = np.mean(X_test, axis=-1, keepdims=True)\n    \n    print(f\"Loaded data shapes: Train {X_train.shape}, Val {X_val.shape}, Test {X_test.shape}\")\n    return X_train, y_train, X_val, y_val, X_test, y_test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T12:56:40.869978Z","iopub.execute_input":"2025-05-02T12:56:40.870261Z","iopub.status.idle":"2025-05-02T12:56:40.890610Z","shell.execute_reply.started":"2025-05-02T12:56:40.870239Z","shell.execute_reply":"2025-05-02T12:56:40.889548Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Generator for batch-by-batch processing to save memory\ndef data_generator(images, masks=None, batch_size=BATCH_SIZE):\n    num_samples = len(images)\n    while True:\n        # Shuffle at the beginning of each epoch\n        indices = np.arange(num_samples)\n        np.random.shuffle(indices)\n        \n        for start_idx in range(0, num_samples, batch_size):\n            end_idx = min(start_idx + batch_size, num_samples)\n            batch_indices = indices[start_idx:end_idx]\n            \n            X_batch = images[batch_indices]\n            \n            if masks is not None:\n                y_batch = masks[batch_indices]\n                yield X_batch, y_batch\n            else:\n                yield X_batch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T12:56:40.891585Z","iopub.execute_input":"2025-05-02T12:56:40.891859Z","iopub.status.idle":"2025-05-02T12:56:40.907114Z","shell.execute_reply.started":"2025-05-02T12:56:40.891839Z","shell.execute_reply":"2025-05-02T12:56:40.906254Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Generate masks for cell images using adaptive thresholding\ndef generate_cell_masks(images, batch_size=32):\n    masks = np.zeros_like(images)\n    \n    # Process in batches to avoid memory issues\n    for i in tqdm(range(0, len(images), batch_size), desc=\"Generating cell masks\"):\n        batch_end = min(i + batch_size, len(images))\n        batch_images = images[i:batch_end]\n        \n        for j, img in enumerate(batch_images):\n            # Convert to single channel and scale to 0-255\n            img_2d = img.squeeze()\n            img_2d = (img_2d * 255).astype(np.uint8)\n            \n            # Apply adaptive thresholding\n            binary = cv2.adaptiveThreshold(\n                img_2d, \n                255, \n                cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \n                cv2.THRESH_BINARY_INV, \n                11, \n                2\n            )\n            \n            # Clean up the mask with morphological operations\n            kernel = np.ones((3, 3), np.uint8)\n            opening = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel, iterations=2)\n            \n            # Remove small objects (noise)\n            nb_components, output, stats, centroids = cv2.connectedComponentsWithStats(opening, connectivity=8)\n            sizes = stats[1:, -1]\n            min_size = 30  # Minimum size of cell objects\n            \n            # Clean mask\n            clean_mask = np.zeros_like(output)\n            for k in range(1, nb_components):\n                if sizes[k - 1] >= min_size:\n                    clean_mask[output == k] = 1\n            \n            # Add channel dimension back\n            masks[i+j] = clean_mask.reshape(img.shape)\n    \n    return masks","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T12:56:40.907975Z","iopub.execute_input":"2025-05-02T12:56:40.908241Z","iopub.status.idle":"2025-05-02T12:56:40.928259Z","shell.execute_reply.started":"2025-05-02T12:56:40.908214Z","shell.execute_reply":"2025-05-02T12:56:40.927308Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def extract_roi(images, masks, batch_size=32):\n    roi_images = np.zeros_like(images)\n    \n    for i in tqdm(range(0, len(images), batch_size), desc=\"Extracting ROIs\"):\n        batch_end = min(i + batch_size, len(images))\n        batch_images = images[i:batch_end]\n        batch_masks = masks[i:batch_end]\n        \n        # Apply mask to original image\n        roi_images[i:batch_end] = batch_images * batch_masks\n    \n    return roi_images","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T12:56:40.929459Z","iopub.execute_input":"2025-05-02T12:56:40.929781Z","iopub.status.idle":"2025-05-02T12:56:40.949652Z","shell.execute_reply.started":"2025-05-02T12:56:40.929725Z","shell.execute_reply":"2025-05-02T12:56:40.948820Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Process individual cells for feature extraction\ndef extract_cell_features(image, mask):\n    # Label individual cells\n    labeled_mask = measure.label(mask.squeeze())\n    props = measure.regionprops(labeled_mask)\n    \n    cell_features = []\n    for prop in props:\n        # Get bounding box\n        min_row, min_col, max_row, max_col = prop.bbox\n        \n        # Extract cell\n        cell = image.squeeze()[min_row:max_row, min_col:max_col]\n        \n        # Calculate features\n        mean_intensity = prop.mean_intensity\n        area = prop.area\n        perimeter = prop.perimeter\n        eccentricity = prop.eccentricity\n        \n        cell_features.append({\n            'cell_image': cell,\n            'mean_intensity': mean_intensity,\n            'area': area,\n            'perimeter': perimeter,\n            'eccentricity': eccentricity\n        })\n    \n    return cell_features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T12:56:40.950483Z","iopub.execute_input":"2025-05-02T12:56:40.950706Z","iopub.status.idle":"2025-05-02T12:56:40.971788Z","shell.execute_reply.started":"2025-05-02T12:56:40.950689Z","shell.execute_reply":"2025-05-02T12:56:40.970883Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Save some example segmentation results for visualization\ndef save_segmentation_examples(images, masks, rois, filename=\"cell_segmentation_examples.png\"):\n    fig, axes = plt.subplots(5, 3, figsize=(15, 25))\n    \n    for i in range(5):\n        # Original image\n        axes[i, 0].imshow(images[i].squeeze(), cmap='gray')\n        axes[i, 0].set_title('Original Cell Image')\n        axes[i, 0].axis('off')\n        \n        # Mask\n        axes[i, 1].imshow(masks[i].squeeze(), cmap='gray')\n        axes[i, 1].set_title('Cell Segmentation Mask')\n        axes[i, 1].axis('off')\n        \n        # ROI\n        axes[i, 2].imshow(rois[i].squeeze(), cmap='gray')\n        axes[i, 2].set_title('Extracted Cell ROI')\n        axes[i, 2].axis('off')\n    \n    plt.tight_layout()\n    plt.savefig(os.path.join(OUTPUT_DIR, filename))\n    plt.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T12:56:40.972694Z","iopub.execute_input":"2025-05-02T12:56:40.972985Z","iopub.status.idle":"2025-05-02T12:56:40.989564Z","shell.execute_reply.started":"2025-05-02T12:56:40.972964Z","shell.execute_reply":"2025-05-02T12:56:40.988697Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Create a visualization showing cell segmentation outlines\ndef save_cell_contour_visualization(images, masks, filename=\"cell_contours.png\"):\n    fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n    \n    for i in range(min(9, len(images))):\n        row, col = i // 3, i % 3\n        \n        # Get original image\n        img = images[i].squeeze()\n        mask = masks[i].squeeze()\n        \n        # Create RGB visualization\n        img_rgb = np.stack([img, img, img], axis=-1)\n        \n        # Find contours\n        contours, _ = cv2.findContours(\n            (mask * 255).astype(np.uint8),\n            cv2.RETR_EXTERNAL,\n            cv2.CHAIN_APPROX_SIMPLE\n        )\n        \n        # Draw contours on RGB image\n        cv2.drawContours(img_rgb, contours, -1, (0, 1, 0), 1)  # Green contours\n        \n        # Display\n        axes[row, col].imshow(img_rgb)\n        axes[row, col].set_title(f'Cell Sample {i+1}')\n        axes[row, col].axis('off')\n    \n    plt.tight_layout()\n    plt.savefig(os.path.join(OUTPUT_DIR, filename))\n    plt.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T12:56:40.990442Z","iopub.execute_input":"2025-05-02T12:56:40.990680Z","iopub.status.idle":"2025-05-02T12:56:41.012158Z","shell.execute_reply.started":"2025-05-02T12:56:40.990662Z","shell.execute_reply":"2025-05-02T12:56:41.011322Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def predict_in_batches(model, images, batch_size=BATCH_SIZE):\n    num_samples = len(images)\n    predictions = np.zeros_like(images)\n    \n    for i in tqdm(range(0, num_samples, batch_size), desc=\"Predicting\"):\n        batch_end = min(i + batch_size, num_samples)\n        batch_images = images[i:batch_end]\n        \n        batch_predictions = model.predict(batch_images, verbose=0)\n        predictions[i:batch_end] = batch_predictions\n        \n        # Clear GPU memory after each batch\n        tf.keras.backend.clear_session()\n    \n    return predictions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T12:56:41.012933Z","iopub.execute_input":"2025-05-02T12:56:41.013263Z","iopub.status.idle":"2025-05-02T12:56:41.029477Z","shell.execute_reply.started":"2025-05-02T12:56:41.013241Z","shell.execute_reply":"2025-05-02T12:56:41.028419Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"def main():\n    # Load preprocessed data\n    print(\"Loading preprocessed data...\")\n    X_train, y_train, X_val, y_val, X_test, y_test = load_preprocessed_data()\n    \n    # Create or load U-Net model\n    if os.path.exists(SEGMENTATION_MODEL_PATH):\n        print(f\"Loading existing U-Net model from {SEGMENTATION_MODEL_PATH}\")\n        custom_objects = {'iou_metric': iou_metric}\n        model = tf.keras.models.load_model(SEGMENTATION_MODEL_PATH, custom_objects=custom_objects)\n    else:\n        print(\"Building and training lightweight U-Net segmentation model...\")\n        model = build_unet_model(input_size=(IMG_SIZE, IMG_SIZE, 1))\n        \n        \n        # Generate training masks for a subset to save memory\n        print(\"Generating cell masks...\")\n        # Use 50% of training data to reduce memory usage\n        train_subset_size = len(X_train) // 2\n        train_subset = X_train[:train_subset_size]\n        train_masks = generate_cell_masks(train_subset)\n        val_masks = generate_cell_masks(X_val)\n        \n        # Set up callbacks\n        callbacks = [\n            ModelCheckpoint(SEGMENTATION_MODEL_PATH, save_best_only=True, monitor='val_loss'),\n            EarlyStopping(patience=5, monitor='val_loss'),  # Reduced patience\n            ReduceLROnPlateau(factor=0.2, patience=3, min_lr=1e-6, monitor='val_loss')  # Modified\n        ]\n        \n        # Create data generators\n        train_gen = data_generator(train_subset, train_masks, batch_size=BATCH_SIZE)\n        val_gen = data_generator(X_val, val_masks, batch_size=BATCH_SIZE)\n        \n        # Train the model with generators\n        history = model.fit(\n            train_gen,\n            steps_per_epoch=len(train_subset) // BATCH_SIZE,\n            epochs=EPOCHS,\n            validation_data=val_gen,\n            validation_steps=len(X_val) // BATCH_SIZE,\n            callbacks=callbacks\n        )\n        \n        # Plot training history\n        plt.figure(figsize=(12, 4))\n        plt.subplot(1, 2, 1)\n        plt.plot(history.history['loss'], label='Train Loss')\n        plt.plot(history.history['val_loss'], label='Validation Loss')\n        plt.title('Loss')\n        plt.legend()\n        \n        plt.subplot(1, 2, 2)\n        plt.plot(history.history['accuracy'], label='Train Accuracy')\n        plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n        plt.title('Accuracy')\n        plt.legend()\n        \n        plt.tight_layout()\n        plt.savefig(os.path.join(OUTPUT_DIR, 'cell_training_history.png'))\n        plt.close()\n    \n    # Predict segmentation masks in batches\n    print(\"Predicting cell segmentation masks in batches...\")\n    \n    # Process smaller subsets of the data to avoid OOM errors\n    # For example, use 25% of training data for visualization\n    train_viz_size = min(100, len(X_train))\n    val_viz_size = min(100, len(X_val))\n    test_viz_size = min(100, len(X_test))\n    \n    train_pred_masks = predict_in_batches(model, X_train[:train_viz_size])\n    val_pred_masks = predict_in_batches(model, X_val[:val_viz_size])\n    test_pred_masks = predict_in_batches(model, X_test[:test_viz_size])\n    \n    # Clear memory\n    tf.keras.backend.clear_session()\n    \n    # Convert predictions to binary masks\n    threshold = 0.5\n    print(\"Converting predictions to binary masks...\")\n    train_binary_masks = (train_pred_masks > threshold).astype(np.float32)\n    val_binary_masks = (val_pred_masks > threshold).astype(np.float32)\n    test_binary_masks = (test_pred_masks > threshold).astype(np.float32)\n    \n    # Post-process the binary masks to clean them up\n    print(\"Post-processing masks...\")\n    for i in range(len(train_binary_masks)):\n        # Remove small objects\n        binary = morphology.remove_small_objects(train_binary_masks[i].squeeze().astype(bool), min_size=30)\n        # Fill holes\n        binary = morphology.binary_closing(binary, morphology.disk(3))\n        binary = ndimage.binary_fill_holes(binary)\n        train_binary_masks[i] = binary.astype(np.float32).reshape(train_binary_masks[i].shape)\n    \n    for i in range(len(val_binary_masks)):\n        binary = morphology.remove_small_objects(val_binary_masks[i].squeeze().astype(bool), min_size=30)\n        binary = morphology.binary_closing(binary, morphology.disk(3))\n        binary = ndimage.binary_fill_holes(binary)\n        val_binary_masks[i] = binary.astype(np.float32).reshape(val_binary_masks[i].shape)\n    \n    for i in range(len(test_binary_masks)):\n        binary = morphology.remove_small_objects(test_binary_masks[i].squeeze().astype(bool), min_size=30)\n        binary = morphology.binary_closing(binary, morphology.disk(3))\n        binary = ndimage.binary_fill_holes(binary)\n        test_binary_masks[i] = binary.astype(np.float32).reshape(test_binary_masks[i].shape)\n    \n    # Extract ROIs in batches\n    print(\"Extracting cell ROIs...\")\n    train_roi = extract_roi(X_train[:train_viz_size], train_binary_masks)\n    val_roi = extract_roi(X_val[:val_viz_size], val_binary_masks)\n    test_roi = extract_roi(X_test[:test_viz_size], test_binary_masks)\n    \n    # Save visualizations before full dataset processing\n    print(\"Saving cell segmentation examples...\")\n    save_segmentation_examples(X_train[:5], train_binary_masks[:5], train_roi[:5], \"train_cell_segmentation.png\")\n    save_segmentation_examples(X_val[:5], val_binary_masks[:5], val_roi[:5], \"val_cell_segmentation.png\")\n    save_segmentation_examples(X_test[:5], test_binary_masks[:5], test_roi[:5], \"test_cell_segmentation.png\")\n    \n    # Save contour visualization\n    save_cell_contour_visualization(X_train[:9], train_binary_masks[:9], \"cell_contours_train.png\")\n    \n    # Calculate segmentation metrics on a small sample\n    print(\"Calculating segmentation metrics...\")\n    sample_size = min(50, len(val_binary_masks))\n    \n    # Generate reference masks for a subset\n    reference_masks = generate_cell_masks(X_val[:sample_size])\n    \n    dice_scores = []\n    iou_scores = []\n    \n    for i in range(sample_size):\n        pred = val_binary_masks[i].squeeze().astype(np.bool_)\n        true = reference_masks[i].squeeze().astype(np.bool_)\n        \n        intersection = np.logical_and(pred, true).sum()\n        union = np.logical_or(pred, true).sum()\n        \n        # IoU\n        iou = intersection / (union + 1e-7)\n        iou_scores.append(iou)\n        \n        # Dice coefficient\n        dice = (2 * intersection) / (pred.sum() + true.sum() + 1e-7)\n        dice_scores.append(dice)\n    \n    # Save metrics\n    with open(os.path.join(OUTPUT_DIR, 'segmentation_metrics.txt'), 'w') as f:\n        f.write(f\"Mean IoU: {np.mean(iou_scores):.4f}\\n\")\n        f.write(f\"Mean Dice: {np.mean(dice_scores):.4f}\\n\")\n    \n    # If memory allows, process the full datasets\n    print(\"Processing full datasets in batches...\")\n    \n    # Create an optimized function that processes full datasets in chunks and saves immediately\n    def process_and_save_full_dataset(X, y, dataset_name, batch_size=16):\n        \"\"\"Process a full dataset in small batches and save immediately to avoid memory issues\"\"\"\n        total_batches = len(X) // batch_size + (1 if len(X) % batch_size > 0 else 0)\n        \n        # Create NPZ files for appending\n        output_file = os.path.join(OUTPUT_DIR, f'cell_roi_{dataset_name}_data.npz')\n        \n        # Process in batches\n        for batch_idx in tqdm(range(total_batches), desc=f\"Processing {dataset_name} dataset\"):\n            start_idx = batch_idx * batch_size\n            end_idx = min(start_idx + batch_size, len(X))\n            \n            # Get batch\n            X_batch = X[start_idx:end_idx]\n            y_batch = y[start_idx:end_idx]\n            \n            # Predict masks\n            pred_masks = model.predict(X_batch, verbose=0)\n            binary_masks = (pred_masks > threshold).astype(np.float32)\n            \n            # Post-process masks\n            for i in range(len(binary_masks)):\n                binary = morphology.remove_small_objects(binary_masks[i].squeeze().astype(bool), min_size=30)\n                binary = morphology.binary_closing(binary, morphology.disk(3))\n                binary = morphology.binary_fill_holes(binary)\n                binary_masks[i] = binary.astype(np.float32).reshape(binary_masks[i].shape)\n            \n            # Extract ROIs\n            roi_batch = X_batch * binary_masks\n            \n            # Save this batch to a temporary file\n            batch_file = os.path.join(OUTPUT_DIR, f'batch_{dataset_name}_{batch_idx}.npz')\n            np.savez_compressed(batch_file, X=roi_batch, y=y_batch)\n            \n            # Clear memory\n            del pred_masks, binary_masks, roi_batch\n            tf.keras.backend.clear_session()\n        \n        # Combine all batch files into one dataset file\n        combined_X = []\n        combined_y = []\n        \n        for batch_idx in range(total_batches):\n            batch_file = os.path.join(OUTPUT_DIR, f'batch_{dataset_name}_{batch_idx}.npz')\n            if os.path.exists(batch_file):\n                data = np.load(batch_file)\n                combined_X.append(data['X'])\n                combined_y.append(data['y'])\n                # Delete temporary file\n                os.remove(batch_file)\n        \n        # Save combined dataset\n        combined_X = np.concatenate(combined_X, axis=0)\n        combined_y = np.concatenate(combined_y, axis=0)\n        np.savez_compressed(output_file, X=combined_X, y=combined_y)\n        \n        print(f\"Saved {dataset_name} dataset with {len(combined_X)} samples\")\n    \n    # Try to process the full datasets\n    try:\n        # Use very small batch size for full dataset processing\n        full_batch_size = 8\n        \n        # Process with error handling\n        try:\n            process_and_save_full_dataset(X_train, y_train, 'train', batch_size=full_batch_size)\n        except Exception as e:\n            print(f\"Error processing train dataset: {e}\")\n            print(\"Saving only visualized subset instead\")\n            np.savez_compressed(os.path.join(OUTPUT_DIR, 'cell_roi_train_data.npz'), \n                                X=train_roi, y=y_train[:train_viz_size])\n        \n        try:\n            process_and_save_full_dataset(X_val, y_val, 'val', batch_size=full_batch_size)\n        except Exception as e:\n            print(f\"Error processing validation dataset: {e}\")\n            print(\"Saving only visualized subset instead\")\n            np.savez_compressed(os.path.join(OUTPUT_DIR, 'cell_roi_val_data.npz'), \n                                X=val_roi, y=y_val[:val_viz_size])\n        \n        try:\n            process_and_save_full_dataset(X_test, y_test, 'test', batch_size=full_batch_size)\n        except Exception as e:\n            print(f\"Error processing test dataset: {e}\")\n            print(\"Saving only visualized subset instead\")\n            np.savez_compressed(os.path.join(OUTPUT_DIR, 'cell_roi_test_data.npz'), \n                                X=test_roi, y=y_test[:test_viz_size])\n    \n    except Exception as e:\n        print(f\"Error in full dataset processing: {e}\")\n        print(\"Saving visualization subsets only\")\n        np.savez_compressed(os.path.join(OUTPUT_DIR, 'cell_roi_train_data.npz'), X=train_roi, y=y_train[:train_viz_size])\n        np.savez_compressed(os.path.join(OUTPUT_DIR, 'cell_roi_val_data.npz'), X=val_roi, y=y_val[:val_viz_size])\n        np.savez_compressed(os.path.join(OUTPUT_DIR, 'cell_roi_test_data.npz'), X=test_roi, y=y_test[:test_viz_size])\n    \n    print(\"Cell ROI extraction complete! Results saved to:\", OUTPUT_DIR)\n    print(\"You can now use the extracted ROIs for your transfer learning hybrid approach.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T12:56:41.030542Z","iopub.execute_input":"2025-05-02T12:56:41.030827Z","iopub.status.idle":"2025-05-02T12:56:41.063083Z","shell.execute_reply.started":"2025-05-02T12:56:41.030794Z","shell.execute_reply":"2025-05-02T12:56:41.062106Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T12:56:41.064069Z","iopub.execute_input":"2025-05-02T12:56:41.064415Z","iopub.status.idle":"2025-05-02T13:03:26.286271Z","shell.execute_reply.started":"2025-05-02T12:56:41.064389Z","shell.execute_reply":"2025-05-02T13:03:26.285190Z"}},"outputs":[{"name":"stdout","text":"Loading preprocessed data...\nLoaded data shapes: Train (17755, 224, 224, 1), Val (3134, 224, 224, 1), Test (3687, 224, 224, 1)\nBuilding and training lightweight U-Net segmentation model...\nGenerating cell masks...\n","output_type":"stream"},{"name":"stderr","text":"Generating cell masks: 100%|██████████| 278/278 [00:13<00:00, 20.07it/s]\nGenerating cell masks: 100%|██████████| 98/98 [00:04<00:00, 20.10it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 1/2\n","output_type":"stream"},{"name":"stderr","text":"\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1746190676.844151     969 service.cc:148] XLA service 0x7f9394028a80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1746190676.844991     969 service.cc:156]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\nI0000 00:00:1746190678.303536     969 cuda_dnn.cc:529] Loaded cuDNN version 90300\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m   3/2219\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:00\u001b[0m 55ms/step - accuracy: 0.9852 - iou_metric: 0.0082 - loss: 0.6796 ","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1746190695.457430     969 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m2219/2219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 54ms/step - accuracy: 0.9935 - iou_metric: 9.5688e-05 - loss: nan - val_accuracy: 0.9937 - val_iou_metric: 0.0000e+00 - val_loss: nan - learning_rate: 1.0000e-04\nEpoch 2/2\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/callbacks/model_checkpoint.py:261: RuntimeWarning: invalid value encountered in less\n  if self.monitor_op(current, self.best):\n/usr/local/lib/python3.11/dist-packages/keras/src/callbacks/reduce_lr_on_plateau.py:94: RuntimeWarning: invalid value encountered in less\n  self.monitor_op = lambda a, b: np.less(a, b - self.min_delta)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m2219/2219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 54ms/step - accuracy: 0.9936 - iou_metric: 0.0000e+00 - loss: nan - val_accuracy: 0.9937 - val_iou_metric: 0.0000e+00 - val_loss: nan - learning_rate: 1.0000e-04\nPredicting cell segmentation masks in batches...\n","output_type":"stream"},{"name":"stderr","text":"Predicting: 100%|██████████| 25/25 [00:14<00:00,  1.77it/s]\nPredicting: 100%|██████████| 25/25 [00:13<00:00,  1.91it/s]\nPredicting: 100%|██████████| 25/25 [00:13<00:00,  1.90it/s]\n/tmp/ipykernel_936/1631603577.py:82: RuntimeWarning: invalid value encountered in greater\n  train_binary_masks = (train_pred_masks > threshold).astype(np.float32)\n/tmp/ipykernel_936/1631603577.py:83: RuntimeWarning: invalid value encountered in greater\n  val_binary_masks = (val_pred_masks > threshold).astype(np.float32)\n/tmp/ipykernel_936/1631603577.py:84: RuntimeWarning: invalid value encountered in greater\n  test_binary_masks = (test_pred_masks > threshold).astype(np.float32)\n","output_type":"stream"},{"name":"stdout","text":"Converting predictions to binary masks...\nPost-processing masks...\nExtracting cell ROIs...\n","output_type":"stream"},{"name":"stderr","text":"Extracting ROIs: 100%|██████████| 4/4 [00:00<00:00, 93.28it/s]\nExtracting ROIs: 100%|██████████| 4/4 [00:00<00:00, 176.59it/s]\nExtracting ROIs: 100%|██████████| 4/4 [00:00<00:00, 180.02it/s]","output_type":"stream"},{"name":"stdout","text":"Saving cell segmentation examples...\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Calculating segmentation metrics...\n","output_type":"stream"},{"name":"stderr","text":"Generating cell masks: 100%|██████████| 2/2 [00:00<00:00, 26.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Processing full datasets in batches...\n","output_type":"stream"},{"name":"stderr","text":"Processing train dataset:   0%|          | 0/2220 [00:04<?, ?it/s]\n","output_type":"stream"},{"name":"stdout","text":"Error processing train dataset: module 'skimage.morphology' has no attribute 'binary_fill_holes'\nSaving only visualized subset instead\n","output_type":"stream"},{"name":"stderr","text":"Processing val dataset:   0%|          | 0/392 [00:00<?, ?it/s]\n","output_type":"stream"},{"name":"stdout","text":"Error processing validation dataset: module 'skimage.morphology' has no attribute 'binary_fill_holes'\nSaving only visualized subset instead\n","output_type":"stream"},{"name":"stderr","text":"Processing test dataset:   0%|          | 0/461 [00:00<?, ?it/s]\n","output_type":"stream"},{"name":"stdout","text":"Error processing test dataset: module 'skimage.morphology' has no attribute 'binary_fill_holes'\nSaving only visualized subset instead\nCell ROI extraction complete! Results saved to: ./cell_roi_extracted\nYou can now use the extracted ROIs for your transfer learning hybrid approach.\n","output_type":"stream"}],"execution_count":19}]}